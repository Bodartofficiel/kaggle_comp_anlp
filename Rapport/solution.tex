\subsection{TF-IDF}

TF-IDF est une méthode statistique utilisée pour évaluer l'importance d'un mot dans un document par rapport à une collection de documents. Elle combine deux métriques : la fréquence du terme (TF) et la fréquence inverse du document (IDF). La fréquence du terme mesure combien de fois un mot apparaît dans un document, tandis que l'IDF diminue l'importance des mots qui apparaissent fréquemment dans tous les documents.

Pour notre utilisation, TF-IDF nous permet de créer des vecteurs de représentation à partir des textes, puis d'entraîner un classifieur pour évaluer la langue du texte. Dans notre cas, nous avons utilisé un classifieur SVM car c'est celui qui, théoriquement et expérimentalement, donne les meilleurs résultats (voir \cite{baldwin2010language}).

Cette approche nous a donné des résultats assez satisfaisants mais pas suffisants pour notre cas d'utilisation et s'est laissé largement dépasser par les autres approches, certainement car elle ne prend pas en compte les relations entre les mots et les contextes dans lesquels ils apparaissent.

\subsection{FastText}

FastText est une bibliothèque développée par Facebook \cite{joulin2017bag} pour la représentation de mots et l'apprentissage de classificateurs de texte. Comme vu en cours, elle repose sur des modèles de type n-grammes pour capturer des informations sur les sous-mots, ce qui permet de mieux gérer les mots rares et les fautes d'orthographe.

FastText est particulièrement efficace pour l'identification de langue car il peut capturer des caractéristiques spécifiques à chaque langue au niveau des sous-mots. Il a donné de très bons résultats dans notre étude, avec des scores un tout petit peu inférieur aux modèles de langage neuronaux (voir ci-dessous), mais avec des temps d'entraînement et d'inférence très rapides en comparaison.

Pour son entraînement, nous avons dans un premier temps pris les hyperparamètres donnés dans l'article GlotLID \cite{Kargaran_2023}, un article présentant un modèle classifiant plus de 1800 langues, puis nous avons fait une recherche d'hyperparamètres aux alentours de ces derniers. 

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lcc}
        \hline
        \textbf{Argument} & \textbf{Description} & \textbf{Valeur} \\
        \hline
        -minCount & Occurrences min mot & 1000 \\
        -minCountLabel & Occurrences min label & 0 \\
        -wordNgrams & Max n-grammes mots & 1 \\
        -bucket & Nb de buckets & $10^6$ \\
        -minn & Min. n-gram caractères & 1 \\
        -maxn & Max. n-gram caractères & 5 \\
        -loss & Perte & softmax \\
        -dim & Taille vecteurs & 256 \\
        -epoch & Nombre d'époques & 9 \\
        -lr & Taux apprentissage & 1.5 \\
        \hline
    \end{tabular}
    \caption{Hyperparamètres d'entraînement du modèle FastText}
    \label{tab:glotlid-m-hyperparams}
\end{table}

\subsection{mBERT}

mBERT (Multilingual BERT) est la version multilingue du modèle BERT \cite{devlin2019bert}, qui est un modèle de langage basé sur des transformers. Sa particularité est qu'il est pré-entraîné sur la tâche de Masked Language Model, sur un vaste corpus de textes dans 104 langues, ce qui lui permet de comprendre et de générer des représentations contextuelles pour des textes dans différentes langues.

Ce modèle fut notre première approche de langage neuronal pour ce projet.  Nous avons de même utilisé le tokenizer correspondant à mBERT, comprenant tous les caractères de notre corpus, et nous n'avons pas mis de limite de taille pour le nombre de tokens. Nous avons d'abord essayé une méthode de transfert (transfer learning) pour adapter mBERT à notre tâche, mais cela s'est avéré insuffisant et il a finalement fallu ajuster les poids du modèle (fine-tuning) pour obtenir des résultats satisfaisants sur 10 epochs, en utilisant 85\% du dataset fourni en train, et une dernière epoch en utilisant la totalité du corpus, pour utiliser le plein potentiel de nos données. L'inconvénient de cette méthode est qu'elle est très lourde en calculs (plusieurs heures d'entraînement).

Finalement, mBERT a donné des résultats satisfaisants mais s'est logiquement laissé dépasser par XLM-RoBERTa (voir ci-dessous), qui possède plus de paramètres.

\subsection{XLM-RoBERTa}

XLM-RoBERTa \cite{conneau2020unsupervised} est une variante optimisée de BERT \cite{devlin2019bert} qui utilise un processus d'entraînement plus robuste et une plus grande quantité de données sur 100 langues. Les améliorations incluent l'entraînement sur des lots de données plus grands, l'utilisation de plus de données d'entraînement, et l'optimisation des hyperparamètres. C'est donc très logiquement que nos résultats suivent l'amélioration promise par les chercheurs à son origine et qu'il surpasse mBERT.

Notre utilisation reprend la même logique que pour mBERT avec une nette amélioration des performances, mais avec un coût en temps d'entraînement et d'inférence plus élevé. Pour palier à cela, nous avons décidé de restreindre chaque phrase à 128 tokens, avec un padding si la phrase contient moins de 128 tokens, et un troncage si elle en contient plus. Cela a drastiquement réduit le temps d'entraînement. Nous avons également utilisé un taux d'apprentissage de 2e-5, un batch size de 64, et un nombre d'époques de 10. On a de plus utilisé le tokenizer de XLM-RoBERTa.

\subsection{Combinaison FastText et XLM-RoBERTa}

Il nous est naturellement venu l'envie de combiner nos modèles portant sur des approches différentes pour tenter de profiter des avantages de chaques approches, pour améliorer les performances de notre solution. La combinaison qui nous a donné les meilleurs résultats après divers essais est la suivante : si l'inférence avec FastText donne une prédiction avec une probabilité supérieure à 0.99\%, on la garde ; sinon, on utilise XLM-RoBERTa pour donner une prédiction. Nous avons fait une étude de sensibilité sur le seuil de probabilité de FastText, et 0.99\% est le seuil qui nous a donné les meilleurs résultats, et, en effet, les prédictions au dessus de ce seuil ont une accuracy de 99\% sur le val.